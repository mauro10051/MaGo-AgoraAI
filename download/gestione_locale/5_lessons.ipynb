{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ac4f3-c6c8-48f4-8f58-5be58af37ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7fa6c47-a292-4a65-a3b5-1979637e92a6",
   "metadata": {},
   "source": [
    "## Section 5: We use the two new profiles, the concept maps, and the two outlines to generate two interviews/lessons that explore and delve into the initial question from two different perspectives ‚Äî Max's and Samanta's ‚Äî with an educational and informative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6f3df0-ce2f-44f5-8aaa-65be13b5102e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Sessioni terminate con successo\n",
      "‚úîÔ∏è Directory di lavoro impostata a: C:\\Users\\maian\\notebook\\py10\\MaGo\n",
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé§ Pol: Iniziamo la lezione su: Il tempo √® una realt√† oggettiva o una costruzione della mente?\n",
      "\n",
      "\n",
      "üìç Punto 1: Tempo: Realt√† Oggettiva o Costruzione Mentale?\n",
      "ü§ñ Pol: Il tempo, come lo percepiamo e misuriamo, √® una realt√† fisica indipendente dalla nostra mente, o una costruzione puramente concettuale, un modo per dare senso all'esperienza?\n",
      "\n",
      "ü§ñ Max: Come docente esperto, posso affermare che il tempo √® un fenomeno complesso che si colloca a cavallo tra realt√† fisica e costruzione concettuale.\n",
      "\n",
      "**In termini fisici, il tempo esiste indipendentemente da noi.** La fisica moderna, con la relativit√† di Einstein, ci dice che il tempo √® intrecciato con lo spazio, formando lo spaziotempo, e che la sua velocit√† pu√≤ variare a seconda della gravit√† e della velocit√† relativa.  Ad esempio, un orologio in cima a una montagna ticchetta leggermente pi√π velocemente di uno a livello del mare, a causa della diversa forza gravitazionale. Questo √® un fatto misurabile e verificabile.\n",
      "\n",
      "**Tuttavia, la *percezione* del tempo √® fortemente influenzata dalla nostra mente.**  L'esperienza soggettiva del tempo pu√≤ dilatarsi o contrarsi a seconda delle emozioni, dell'attenzione e delle attivit√† che svolgiamo.  Un'ora passata a divertirsi sembra volare, mentre un'ora di attesa in fila pu√≤ sembrare interminabile. La memoria gioca un ruolo cruciale: tendiamo a ricordare gli eventi in una sequenza temporale, ma la sua accuratezza √® spesso distorta.\n",
      "\n",
      "In conclusione, il tempo *oggettivo* √® una dimensione fisica, mentre la nostra *esperienza* del tempo √® una costruzione mentale che la interpreta e la d√† un significato.\n",
      "\n",
      "\n",
      "üìç Punto 2: Tempo nella Fisica Newtoniana: Assolutezza e Universalit√†\n",
      "ü§ñ Pol: In che modo il concetto newtoniano di tempo assoluto e universale, intrinseco all'universo e indipendente dall'osservatore, si concilia con le nostre moderne intuizioni sulla relativit√† e la sua dipendenza dal sistema di riferimento?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#versione con max che cambia in base al prompt \n",
    "import gradio as gr\n",
    "import ollama\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from time import sleep\n",
    "# Variabili globali\n",
    "\n",
    "sintesi_raccolte = []\n",
    "progetto=[]\n",
    "messages = []  # Lista per memorizzare i messaggi scambiati\n",
    "turno = 0      # Contatore per i turni\n",
    "domanda_osservatore = None  # Domanda dell'osservatore\n",
    "\n",
    "\n",
    "modello_max='gemma3:12b'\n",
    "modello_sam='gemma3:12b'\n",
    "modello_pol='gemma3:12b'\n",
    "\n",
    "\n",
    "def clean_ollama_sessions():\n",
    "    \"\"\"Versione per Jupyter\"\"\"\n",
    "    try:\n",
    "        output = !ollama kill 2>&1\n",
    "        if \"no running sessions\" in str(output).lower():\n",
    "            print(\"‚úÖ Nessuna sessione attiva da terminare\")\n",
    "        else:\n",
    "            print(\"üõë Sessioni terminate con successo\")\n",
    "    except:\n",
    "        print(\"‚ùå Comando ollama non trovato\")\n",
    "\n",
    "# Esegui all'inizio del notebook\n",
    "clean_ollama_sessions()\n",
    "\n",
    "\n",
    "css = \"\"\"\n",
    "button:active {\n",
    "    background-color: #ff6600 !important;  /* colore di sfondo quando premuto */\n",
    "    color: white !important;               /* colore del testo quando premuto */\n",
    "    transform: scale(0.97);                /* leggero effetto di \"pressione\" */\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Carica il percorso salvato\n",
    "CONFIG_PATH = Path(os.getenv(\"APPDATA\")) / \"MaGo\" / \"config.json\"\n",
    "\n",
    "def load_project_path():\n",
    "    \"\"\"Carica il percorso dal file di configurazione\"\"\"\n",
    "    if not CONFIG_PATH.exists():\n",
    "        return None\n",
    "    \n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return config.get(\"project_path\")\n",
    "\n",
    "# Verifica all'avvio\n",
    "#project_path = load_project_path()\n",
    "# üî• QUI: cast a Path\n",
    "project_path = Path(load_project_path())  # <-- Fondamentale!\n",
    "\n",
    "if not project_path.exists():\n",
    "    raise RuntimeError(\"Percorso non valido!\")\n",
    "\n",
    "os.chdir(project_path)\n",
    "\n",
    "if not project_path or not Path(project_path).exists():\n",
    "    raise RuntimeError(\n",
    "        \"üõë Percorso non configurato o non valido!\\n\"\n",
    "        \"Esegui prima il notebook 'installer.ipynb' e imposta la cartella corretta.\"\n",
    "    )\n",
    "\n",
    "# Imposta la directory di lavoro\n",
    "os.chdir(project_path)\n",
    "print(f\"‚úîÔ∏è Directory di lavoro impostata a: {project_path}\")\n",
    "\n",
    "\n",
    "global project_path\n",
    "\n",
    "\n",
    "# Variabile per salvare lo scenario selezionato\n",
    "scena_selezionata = gr.State(value=None)\n",
    "\n",
    "def aggiorna_scena(scelta):\n",
    "    return scelta  # Restituisce il valore selezionato\n",
    "\n",
    "messages = []  # Lista dei messaggi globali\n",
    "\n",
    "def interazione_osservatore(domanda, voto):\n",
    "    global messages\n",
    "    messages.append({'role': 'user', 'content': f\"Domanda dall'osservatore: {domanda}\"})\n",
    "\n",
    "    if voto < 3:\n",
    "        azione = \"approfondire\"\n",
    "    else:\n",
    "        azione = \"concludere\"\n",
    "\n",
    "    messages.append({'role': 'system', 'content': f\"Decisione: {azione}\"})\n",
    "\n",
    "    return azione\n",
    "\n",
    "def inizializza_lezione_max(prof1, domanda, scaletta,  map1,lingua):\n",
    "   \n",
    "\n",
    "    messages = [\n",
    "    {'role': 'system','content': f\"Tu sei Max, un docente  esperto in '{prof1}'. Tu sei Max  e hai solide competenze e sai spiegare concetti complessi in modo semplice, ordinato e coinvolgente. \"},\n",
    "     {'role': 'system','content':f\"Tu Max sei un bravo divulgatore scientifico: chiaro, diretto, logico.Hai 20 anni di esperienza di insegnamento e parli davanti a studenti motivati che ti seguono con grande attenzione e interesse\" },\n",
    "     {'role': 'system','content':f\" Il tuo compito Max √® di approfondire questo tema ¬´{domanda}¬ª rispondendo alle domande che ti vengono poste in modo esteso, divulgativo e brillante, ma soprattutto appassionato  \" },\n",
    "     {'role': 'system','content':f\"Tu Pol  sei un moderatore e devi intervistare Max. Tu Pol per fare le domande a Max hai preparato una traccia '{scaletta}' \" }, \n",
    "     {'role': 'system','content':f\" Il tuo compito Pol √® quello di  prendere ogni punto della traccia e formulare una domanda per Max ed invitarlo a rispondere in modo chiaro, esauriente ed esteso. Pol se vuoi sottolineare qualche aspetto della risposta di Max lo puoi fare\" },\n",
    "     {'role': 'system','content':f\"Tu Pol Alla fine della lezione dopo avere posto a Max tutte le domande contenute nella scaletta fai un breve riassunto e termina la lezione \" },\n",
    "     {'role': 'system','content':f\"Max, inizia la lezione  dopo avere ringraziato il gruppo di studio Ma&Go che ti ha invitato a tenere questa lezione\"},\n",
    "    {'role': 'system','content':f\" Attenzione la tua risposta deve essere data nella lingua '{lingua}'\"}\n",
    "]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def inizializza_lezione_sam(prof2, domanda,scal2,map2,lingua):\n",
    "    \n",
    " \n",
    "    messages = [\n",
    "    {'role': 'system','content': f\"Tu sei Samanta, una docente  esperta in '{prof2}'. Tu Samanta hai solide competenze e sai spiegare concetti complessi in modo semplice, ordinato e coinvolgente. \"},\n",
    "     {'role': 'system','content':f\"Tu Samanta sei un bravo divulgatore scientifico: chiaro, diretto, logico. Hai 20 anni di esperienza di insegnamento e parli davanti a studenti motivati che ti seguono con grande attenzione e interesse\" },\n",
    "     {'role': 'system','content':f\" Il tuo compito Samanta √® di approfondire questo tema ¬´{domanda}¬ª rispondendo alle domande che ti vengono poste in modo esteso, divulgativo e brillante \" },\n",
    "     {'role': 'system','content':f\"Tu Pol  sei un moderatore del gruppo di studio Ma&Go e sei un grande esperto dei temi contenuti nella  ¬´{domanda}¬ª e devi intervistare Samanta. Tu Pol per fare le domande a Samanta hai preparato una traccia '{scal2}' \" }, \n",
    "     {'role': 'system','content':f\"Il tuo compito Pol √® quello di  prendere ogni punto della traccia. Tu Pol devi rispettare l'ordine dei punti contenuti nella scaletta  e formulare una domanda per Samanta ed invitarla a rispondere in modo chiaro, esauriente ed esteso. \" },\n",
    "     {'role': 'system','content':f\"Tu Pol Alla fine della lezione dopo avere posto a Samanta tutte le domande contenute nella scaletta fai un breve riassunto e termina la lezione.\" },\n",
    "       {'role': 'system','content':f\" Attenzione tu Pol e tu Samanta dovete esprimervi nella lingua '{lingua}'\"}\n",
    "]\n",
    "\n",
    "   \n",
    "    # Qui puoi restituire o processare il messaggio\n",
    "    return messages\n",
    "\n",
    "\n",
    "\n",
    "# Funzione base migliorata\n",
    "def chat_lezione_max(prof1, domanda, scal1, map1, lingua, progetto):\n",
    "    global project_path\n",
    "    \n",
    "    project_dir = project_path / \"progetti\" / progetto\n",
    "    chat_stato = []\n",
    "    messages=[]\n",
    "    \n",
    "    messages = inizializza_lezione_max(prof1, domanda, scal1,map1,lingua)\n",
    "    punti_scaletta = [riga.strip() for riga in scal1.strip().split(\"\\n\") if riga.strip()]\n",
    "\n",
    "    print(f\"\\nüé§ Pol: Iniziamo la lezione su: {domanda}\\n\")\n",
    "\n",
    "    for punto in punti_scaletta:\n",
    "        numero, testo = punto.split(\".\", 1)\n",
    "        numero = numero.strip()\n",
    "        titolo = testo.strip()\n",
    "        print(f\"\\nüìç Punto {numero}: {titolo}\")\n",
    "        \n",
    "        # 1. Pol genera la domanda\n",
    "        prompt_pol = f\"\"\"Sei Pol, il moderatore. Formula una domanda in {lingua} chiara e stimolante su:\n",
    "        {titolo}\n",
    "        Rispondi solo con la domanda.\"\"\"\n",
    "        \n",
    "        response_pol = ollama.chat(\n",
    "            model=modello_pol,\n",
    "            messages=[{'role': 'user', 'content': prompt_pol}]\n",
    "        )\n",
    "        domanda_pol = response_pol['message']['content']\n",
    "        chat_stato.append((f\"ü§ñ Pol\", domanda_pol))\n",
    "        print(f\"ü§ñ Pol: {domanda_pol}\")\n",
    "\n",
    "        # 2. Samanta risponde\n",
    "        prompt_max = f\"\"\"Sei Max, docente esperto. Rispondi in {lingua} in modo chiaro e autorevole a:\n",
    "        {domanda_pol}\n",
    "        - Sii conciso\n",
    "        - Fornisci esempi concreti\n",
    "        - Non ripetere la domanda\"\"\"\n",
    "        \n",
    "        response_max = ollama.chat(\n",
    "            model=modello_max,\n",
    "            messages=[{'role': 'user', 'content': prompt_max}],\n",
    "            options={\"temperature\": 0.7}\n",
    "        )\n",
    "        risposta_max = response_max['message']['content']\n",
    "        chat_stato.append((f\"ü§ñ Max\", risposta_max))\n",
    "        print(f\"ü§ñ Max: {risposta_max}\\n\")\n",
    "\n",
    "    # Salvataggio\n",
    "    with open(project_dir / 'intervista_max.txt', 'w', encoding=\"utf-8\") as file:\n",
    "        for ruolo, messaggio in chat_stato:\n",
    "            file.write(f\"{ruolo}: {messaggio}\\n\")\n",
    "    \n",
    "    return chat_stato\n",
    "\n",
    "    \n",
    "def chat_lezione_sam(prof2, domanda, scal2, map2, lingua, progetto):\n",
    "    global project_path\n",
    "    \n",
    "    project_dir = project_path / \"progetti\" / progetto\n",
    "    chat_stato = []\n",
    "    messages=[]\n",
    "    \n",
    "    messages = inizializza_lezione_sam(prof2, domanda, scal2,map2,lingua)\n",
    "    punti_scaletta = [riga.strip() for riga in scal2.strip().split(\"\\n\") if riga.strip()]\n",
    "\n",
    "    print(f\"\\nüé§ Pol: Iniziamo la lezione su: {domanda}\\n\")\n",
    "\n",
    "    for punto in punti_scaletta:\n",
    "        numero, testo = punto.split(\".\", 1)\n",
    "        numero = numero.strip()\n",
    "        titolo = testo.strip()\n",
    "        print(f\"\\nüìç Punto {numero}: {titolo}\")\n",
    "        \n",
    "        # 1. Pol genera la domanda\n",
    "        prompt_pol = f\"\"\"Sei Pol, il moderatore. Formula una domanda in {lingua} chiara e stimolante su:\n",
    "        {titolo}\n",
    "        Rispondi solo con la domanda.\"\"\"\n",
    "        \n",
    "        response_pol = ollama.chat(\n",
    "            model=modello_pol,\n",
    "            messages=[{'role': 'user', 'content': prompt_pol}]\n",
    "        )\n",
    "        domanda_pol = response_pol['message']['content']\n",
    "        chat_stato.append((f\"ü§ñ Pol\", domanda_pol))\n",
    "        print(f\"ü§ñ Pol: {domanda_pol}\")\n",
    "\n",
    "        # 2. Samanta risponde\n",
    "        prompt_sam = f\"\"\"Sei Samanta, docente esperto. Rispondi in {lingua} in modo chiaro e autorevole a:\n",
    "        {domanda_pol}\n",
    "        - sei didattica nelle tue spiegazioni, tecnica nel linguaggio, ma  appassionata e coinvolgente \n",
    "        - Fornisci esempi concreti\n",
    "        - Non ripetere la domanda\"\"\"\n",
    "        \n",
    "        response_sam = ollama.chat(\n",
    "            model=modello_sam,\n",
    "            messages=[{'role': 'user', 'content': prompt_sam}],\n",
    "            options={\"temperature\": 0.7}\n",
    "        )\n",
    "        risposta_sam = response_sam['message']['content']\n",
    "        chat_stato.append((f\"ü§ñ Samanta\", risposta_sam))\n",
    "        print(f\"ü§ñ Samanta: {risposta_sam}\\n\")\n",
    "\n",
    "    # Salvataggio\n",
    "    with open(project_dir / 'intervista_sam.txt', 'w', encoding=\"utf-8\") as file:\n",
    "        for ruolo, messaggio in chat_stato:\n",
    "            file.write(f\"{ruolo}: {messaggio}\\n\")\n",
    "    \n",
    "    return chat_stato\n",
    "    \n",
    "  #pol formula la domanda \n",
    "def interroga_moderatore_max(nome, titolo, messages, lingua):\n",
    "    prompt_pol = f\"\"\"\n",
    "    Sei Pol, il moderatore. Devi fare UNA domanda alla volta sul tema: ¬´{titolo}¬ª in {lingua}.\n",
    "    - **Non concludere** l'intervista.  \n",
    "    - **Non anticipare** riassunti o finali.  \n",
    "    - Rispondi SOLO con la domanda, senza commenti. \n",
    "    - Non tornare mai sui punti precedenti ad esempio NON tornare sul punto 1.\n",
    "    - Tu Pol puoi chiedere a Max di approfondire un argomento della sua risposta\n",
    "    \"\"\"\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': prompt_pol})\n",
    "    response = ollama.chat(\n",
    "        model=modello_max,\n",
    "        messages=messages,\n",
    "        options={\"temperature\": 0.3}  # Imposta temperatura bassa (0.3-0.5)\n",
    "    )\n",
    "    domanda = response['message']['content']\n",
    "    \n",
    "    # Controllo anti-chiusura anticipata (semplice ma efficace)\n",
    "    if \"FINE\" in domanda.upper() or \"GRAZIE\" in domanda.upper():\n",
    "        domanda = \"Continua con la prossima domanda sul tema.\"\n",
    "    \n",
    "    #print(f\"ü§ñ {nome}: {domanda}\\n\")\n",
    "    messages.append({'role': 'assistant', 'content': domanda})\n",
    "    return domanda, messages\n",
    "\n",
    "\n",
    "\n",
    "def interroga_attore_max(nome, messages,lingua):\n",
    "    prompt_max = f\"\"\"\n",
    "    Sei Max, rispondi alla domanda di Pol in {lingua}.\n",
    "    - **Non concludere** l'intervista.  \n",
    "    - Sei un docente molto apprezzato rispondi alla la domanda di Pol in modo appassionato e autorevole.  \n",
    "    \"\"\"\n",
    "    messages.append({'role': 'user', 'content': prompt_max})\n",
    "    response = ollama.chat(model=modello_max, messages=messages)\n",
    "    risposta = response['message']['content']\n",
    "    print(f\"ü§ñ {nome}: {risposta}\\n\")\n",
    "\n",
    "    messages.append({'role': 'assistant', 'content': risposta})\n",
    "    return risposta, messages\n",
    "\n",
    "    \n",
    "def aggiorna_progetti():\n",
    "    \"\"\"Restituisce la lista delle cartelle dei progetti.\"\"\"\n",
    "    global project_path\n",
    "    project_dir = Path(project_path) / \"progetti\" \n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)  # Crea la directory se non esiste\n",
    "    return [d for d in os.listdir(project_dir) if os.path.isdir(os.path.join(project_dir, d))]\n",
    "\n",
    "def carica_progetto(progetto):\n",
    "    \n",
    "    global project_path\n",
    "\n",
    "    \"\"\"Carica i file prof1.txt, prof2.txt, scal1.txt, scal2.txt e domanda.txt dalla cartella selezionata.\"\"\"\n",
    " \n",
    "    project_dir = project_path / \"progetti\" /progetto  #\n",
    "    \n",
    "    if not os.path.exists(project_dir):\n",
    "        return \"Errore: Progetto non trovato\", \"\", \"\", \"\", \"\"\n",
    "    if not os.path.exists(project_dir):\n",
    "        return \"Errore: Progetto non trovato\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    def leggi_file(nome_file):\n",
    "        path = os.path.join(project_dir, nome_file)\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                contenuto = f.read().strip()\n",
    "                return contenuto if contenuto else \"\"  # Restituisce stringa vuota se il file √® vuoto\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Attenzione: il file '{nome_file}' non √® stato trovato.\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nella lettura del file '{nome_file}': {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    domanda = leggi_file(\"domanda.txt\")\n",
    "    prof1 = leggi_file(\"prof1.txt\")\n",
    "    prof2 = leggi_file(\"prof2.txt\")\n",
    "    map1 = leggi_file(\"map1.txt\")\n",
    "    map2 = leggi_file(\"map2.txt\")\n",
    "    scal1 = leggi_file(\"scal1.txt\")\n",
    "    scal2 = leggi_file(\"scal2.txt\")\n",
    "    lingua= leggi_file(\"lingua.txt\")\n",
    "   \n",
    "    return prof1, prof2, domanda, scal1, scal2,map1,map2,lingua\n",
    "    \n",
    "def invia_domanda(domanda, messages):\n",
    "    global domanda_osservatore\n",
    "    domanda_osservatore=domand\n",
    "    if domanda.strip():\n",
    "        messages.append((\"Osservatore\", domanda))  # Aggiunge la domanda alla chat\n",
    "    return messages  # Ritorna l'output aggiornato per essere mostrato\n",
    "    \n",
    "def avvia_interfaccia():\n",
    "    with gr.Blocks(css=css) as demo:\n",
    "        stile_scelto = gr.State([]) \n",
    "        gr.Markdown(\"# Generate Max and Samanta's Interviews/Lessons\")\n",
    "        def carica_scenario(scelta):\n",
    "         \n",
    "            # Qui puoi fare qualcosa con il valore numerico scelto, come caricare un prompt o avviare una discussione\n",
    "            return f\" {scelta}\"\n",
    "        # Variabile per salvare lo scenario selezionato\n",
    "        scena_selezionata = gr.State(value=None)   \n",
    "        map1_output= gr.State(value=None) \n",
    "        map2_output= gr.State(value=None) \n",
    "        \n",
    "        with gr.Row():\n",
    "            Progetto = gr.Dropdown(label=\"Select a project\", choices=aggiorna_progetti())\n",
    "            btn_carica = gr.Button(\"Load project\")\n",
    "\n",
    "        \n",
    "        domanda_output = gr.Textbox(label=\"QUESTION\", interactive=False)\n",
    "        with gr.Row():\n",
    "             prof1_output = gr.Textbox(label=\"MAX's Profile\", interactive=False)\n",
    "             prof2_output = gr.Textbox(label=\"Samanta's Profile\", interactive=False)\n",
    "           # with gr.Column():\n",
    "        with gr.Row():\n",
    "             scal1_output = gr.Textbox(label=\"MAX's Outline\", interactive=False)\n",
    "             scal2_output = gr.Textbox(label=\"Samanta's Outline\", interactive=False)\n",
    "\n",
    "\n",
    "        lingua = gr.State()\n",
    "     \n",
    "        btn_carica.click(fn=carica_progetto, inputs=[Progetto], \n",
    "        outputs=[prof1_output, prof2_output, domanda_output,scal1_output,scal2_output,map1_output,map2_output,lingua])\n",
    "\n",
    "                \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                btn_lezione_max = gr.Button(\"Max's Lesson\")\n",
    "            with gr.Column(scale=1):  \n",
    "                btn_lezione_sam = gr.Button(\"Samanta's Lesson\")\n",
    "        #gr.Textbox(label=\"Output della lezione\", lines=10, interactive=False)\n",
    "\n",
    "                 \n",
    "       \n",
    "        chat_stato = gr.State([])  # Stato interno della chat\n",
    "        lezione_output_textbox = gr.Textbox(label=\"Lesson Output\", lines=10, interactive=False)\n",
    "     \n",
    "        btn_lezione_max.click(\n",
    "        fn=chat_lezione_max,\n",
    "        inputs=[prof1_output, domanda_output, scal1_output, map1_output,lingua,Progetto],\n",
    "        outputs=lezione_output_textbox\n",
    "        )\n",
    "     \n",
    "        btn_lezione_sam.click(\n",
    "        fn=chat_lezione_sam,\n",
    "        inputs=[prof2_output, domanda_output, scal2_output, map2_output,lingua,Progetto],\n",
    "        outputs=lezione_output_textbox\n",
    "        )\n",
    "\n",
    "       \n",
    "    demo.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    avvia_interfaccia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ce0e1-f3b1-4733-bfa7-f7202877487c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9450edc-d4a6-458f-8413-03718d6f6dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7e5a1-23da-4940-a4f3-740a16760e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
