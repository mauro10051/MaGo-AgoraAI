{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ac4f3-c6c8-48f4-8f58-5be58af37ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7fa6c47-a292-4a65-a3b5-1979637e92a6",
   "metadata": {},
   "source": [
    "# Section 6: We use the two new profiles, the concept maps, and the two outlines to generate debate  that explore and delve into the initial question from two different perspectives ‚Äî Max's and Samanta's ‚Äî with an educational and informative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6f3df0-ce2f-44f5-8aaa-65be13b5102e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Sessioni terminate con successo\n",
      "‚úîÔ∏è Directory di lavoro impostata a: C:\\Users\\maian\\notebook\\py10\\MaGo\n",
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#versione con max che cambia in base al prompt \n",
    "import gradio as gr\n",
    "import ollama\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from time import sleep\n",
    "# Variabili globali\n",
    "\n",
    "sintesi_raccolte = []\n",
    "progetto=[]\n",
    "messages = []  # Lista per memorizzare i messaggi scambiati\n",
    "turno = 0      # Contatore per i turni\n",
    "domanda_osservatore = None  # Domanda dell'osservatore\n",
    "\n",
    "\n",
    "modello_max='gemma3:12b'\n",
    "modello_sam='gemma3:12b'\n",
    "modello_pol='gemma3:12b'\n",
    "\n",
    "\n",
    "def clean_ollama_sessions():\n",
    "    \"\"\"Versione per Jupyter\"\"\"\n",
    "    try:\n",
    "        output = !ollama kill 2>&1\n",
    "        if \"no running sessions\" in str(output).lower():\n",
    "            print(\"‚úÖ Nessuna sessione attiva da terminare\")\n",
    "        else:\n",
    "            print(\"üõë Sessioni terminate con successo\")\n",
    "    except:\n",
    "        print(\"‚ùå Comando ollama non trovato\")\n",
    "\n",
    "# Esegui all'inizio del notebook\n",
    "clean_ollama_sessions()\n",
    "\n",
    "\n",
    "css = \"\"\"\n",
    "button:active {\n",
    "    background-color: #ff6600 !important;  /* colore di sfondo quando premuto */\n",
    "    color: white !important;               /* colore del testo quando premuto */\n",
    "    transform: scale(0.97);                /* leggero effetto di \"pressione\" */\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Carica il percorso salvato\n",
    "CONFIG_PATH = Path(os.getenv(\"APPDATA\")) / \"MaGo\" / \"config.json\"\n",
    "\n",
    "def load_project_path():\n",
    "    \"\"\"Carica il percorso dal file di configurazione\"\"\"\n",
    "    if not CONFIG_PATH.exists():\n",
    "        return None\n",
    "    \n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return config.get(\"project_path\")\n",
    "\n",
    "# Verifica all'avvio\n",
    "#project_path = load_project_path()\n",
    "# üî• QUI: cast a Path\n",
    "project_path = Path(load_project_path())  # <-- Fondamentale!\n",
    "\n",
    "if not project_path.exists():\n",
    "    raise RuntimeError(\"Percorso non valido!\")\n",
    "\n",
    "os.chdir(project_path)\n",
    "\n",
    "if not project_path or not Path(project_path).exists():\n",
    "    raise RuntimeError(\n",
    "        \"üõë Percorso non configurato o non valido!\\n\"\n",
    "        \"Esegui prima il notebook 'installer.ipynb' e imposta la cartella corretta.\"\n",
    "    )\n",
    "\n",
    "# Imposta la directory di lavoro\n",
    "os.chdir(project_path)\n",
    "print(f\"‚úîÔ∏è Directory di lavoro impostata a: {project_path}\")\n",
    "\n",
    "\n",
    "global project_path\n",
    "\n",
    "\n",
    "# Variabile per salvare lo scenario selezionato\n",
    "scena_selezionata = gr.State(value=None)\n",
    "\n",
    "def aggiorna_scena(scelta):\n",
    "    return scelta  # Restituisce il valore selezionato\n",
    "\n",
    "messages = []  # Lista dei messaggi globali\n",
    "\n",
    "def interazione_osservatore(domanda, voto):\n",
    "    global messages\n",
    "    messages.append({'role': 'user', 'content': f\"Domanda dall'osservatore: {domanda}\"})\n",
    "\n",
    "    if voto < 3:\n",
    "        azione = \"approfondire\"\n",
    "    else:\n",
    "        azione = \"concludere\"\n",
    "\n",
    "    messages.append({'role': 'system', 'content': f\"Decisione: {azione}\"})\n",
    "\n",
    "    return azione\n",
    "\n",
    "def genera_scaletta3(prof1, prof2, domanda, scal1, scal2, map1, map2, lingua):\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': f\"Tu sei Pol, un docente esperto in '{prof1}' e '{prof2}'. Tu, Pol, devi preparare una scaletta per organizzare un dibattito con il fisico Max e il filosofo Samanta.\"\n",
    "        },\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': f\"Tu, Pol, devi considerare le mappe concettuali '{map1}' e '{map2}' per estrarre una scaletta che userai per fare domande a Max e a Samanta.\"\n",
    "        },\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                f\"### Regole per costruire la scaletta:\\n\"\n",
    "                f\"1. Inizia con un solo punto introduttivo che presenta il tema: '{domanda}'.\\n\"\n",
    "                f\"2. I punti devono riguardare le conoscenze comuni di '{map1}' e '{map2}'.\\n\"\n",
    "                f\"3. Ogni punto deve essere un titolo sintetico.\\n\"\n",
    "                f\"4. I punti devono essere numerati in ordine logico e progressivo.\\n\"\n",
    "                f\"5. Non aggiungere commenti, spiegazioni o testo introduttivo.\\n\"\n",
    "                f\"6. L‚Äôultimo punto deve essere 'Conclusioni'.\\n\"\n",
    "                f\"7. Rispondi esclusivamente nella lingua: '{lingua}'.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': f\"Attenzione: la tua risposta deve essere data nella lingua '{lingua}'.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def inizializza_lezione_sam(prof1,prof2, domanda,map1,map2,lingua,scal):\n",
    "    \n",
    " \n",
    "    messages = [\n",
    "    {'role': 'system','content': f\"Tu sei Max, un docente  esperto in '{prof1}'. Tu Max hai solide competenze e sai spiegare concetti complessi in modo semplice, ordinato e coinvolgente. \"},\n",
    "     {'role': 'system','content':f\"Tu Max sei un bravo divulgatore scientifico: chiaro, diretto, logico. Hai 20 anni di esperienza di insegnamento e parli davanti a studenti motivati che ti seguono con grande attenzione e interesse\" },\n",
    "     {'role': 'system','content':f\" Il tuo compito Samanta √® di approfondire questo tema ¬´{domanda}¬ª rispondendo alle domande che ti vengono poste in modo esteso, divulgativo e brillante \" },\n",
    "   {'role': 'system','content': f\"Tu sei Samanta, una docente  esperta in '{prof2}'. Tu Samanta hai solide competenze e sai spiegare concetti complessi in modo semplice, ordinato e coinvolgente. \"},\n",
    "     {'role': 'system','content':f\"Tu Samanta sei un bravo divulgatore scientifico: chiaro, diretto, logico. Hai 20 anni di esperienza di insegnamento e parli davanti a studenti motivati che ti seguono con grande attenzione e interesse\" },\n",
    "     {'role': 'system','content':f\" Il tuo compito Samanta √® di approfondire questo tema ¬´{domanda}¬ª rispondendo alle domande che ti vengono poste in modo esteso, divulgativo e brillante \" }, \n",
    "     {'role': 'system','content':f\"Tu Pol  sei un moderatore del gruppo di studio Ma&Go e sei un grande esperto dei temi contenuti nella  ¬´{domanda}¬ª e devi intervistare Max e Samanta. Tu Pol per fare le domande a Max e a Samanta hai preparato una traccia '{scal}' \" }, \n",
    "     {'role': 'system','content':f\"Il tuo compito Pol √® quello di  prendere ogni punto della traccia. Tu Pol devi rispettare l' inordine dei punti contenuti nella scaletta  e formulare una domanda per Max e Samanta ed invitarli a rispondere modo chiaro, esauriente ed esteso. \" },\n",
    "     {'role': 'system','content':f\"Tu Pol Alla fine della lezione dopo avere posto a Samanta tutte le domande contenute nella scaletta fai un breve riassunto e termina la lezione.\" },\n",
    "       {'role': 'system','content':f\" Attenzione tu Pol e tu Max e tu Samanta dovete esprimervi nella lingua '{lingua}'\"}\n",
    "]\n",
    "\n",
    "   \n",
    "    # Qui puoi restituire o processare il messaggio\n",
    "    return messages\n",
    "\n",
    "def genera_scaletta(prof1,prof2, domanda, scal1,scal2, map1,map2, lingua, progetto):\n",
    "    global project_path\n",
    "    messages=[]\n",
    "    chat_stato_max = []\n",
    "    #profilo=comp1\n",
    "    \n",
    "    messages=genera_scaletta3(prof1,prof2, domanda, scal1,scal2, map1,map2, lingua)\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"Rispondi sempre in {lingua}.\"}) \n",
    "    response = ollama.chat(model=modello_pol, messages=messages)\n",
    "    messages.append({'role': 'assistant', 'content': response['message']['content']})\n",
    "    #print(f\" Max: {response['message']['content']}\\n\")\n",
    "    chat_stato_max.append((\"Max\", response['message']['content']))\n",
    "\n",
    "    scal=response['message']['content']\n",
    "\n",
    "            # Percorso dove salvare la scaletta\n",
    "    project_dir = Path(project_path) / \"progetti\" / progetto\n",
    "    \n",
    "    # Scrivi la risposta di Max su un file di testo\n",
    "    with open(project_dir / 'scal3.txt', 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(scal)  # Scrive la risposta nel file\n",
    "\n",
    "    return scal\n",
    "\n",
    "# Funzione base migliorata\n",
    "\n",
    "\n",
    "    \n",
    "def chat_dibattito(prof1,prof2, domanda, map1, map2, lingua, progetto,scal):\n",
    "    global project_path\n",
    "    \n",
    "    project_dir = project_path / \"progetti\" / progetto\n",
    "    chat_stato = []\n",
    "    messages=[]\n",
    "    \n",
    "    messages = inizializza_lezione_sam(prof1,prof2, domanda, map1,map2,lingua,scal)\n",
    "    punti_scaletta = [riga.strip() for riga in scal.strip().split(\"\\n\") if riga.strip()]\n",
    "\n",
    "    print(f\"\\nüé§ Pol: Iniziamo la lezione su: {domanda}\\n\")\n",
    "\n",
    "    for punto in punti_scaletta:\n",
    "        numero, testo = punto.split(\".\", 1)\n",
    "        numero = numero.strip()\n",
    "        titolo = testo.strip()\n",
    "        print(f\"\\nüìç Punto {numero}: {titolo}\")\n",
    "        \n",
    "        # 1. Pol genera la domanda\n",
    "        prompt_pol = f\"\"\"Sei Pol, il moderatore. Formula una domanda in {lingua} chiara e stimolante su:\n",
    "        {titolo} prima a Max e poi a Samanta\n",
    "        Rispondi solo con la domanda.\"\"\"\n",
    "        \n",
    "        response_pol = ollama.chat(\n",
    "            model=modello_pol,\n",
    "            messages=[{'role': 'user', 'content': prompt_pol}]\n",
    "        )\n",
    "        domanda_pol = response_pol['message']['content']\n",
    "        chat_stato.append((f\"ü§ñ Pol\", domanda_pol))\n",
    "        print(f\"ü§ñ Pol: {domanda_pol}\")\n",
    "\n",
    "        # 2. Max risponde\n",
    "        prompt_max = f\"\"\"Sei Max, docente esperto. Rispondi in {lingua} in modo chiaro e autorevole a:\n",
    "        {domanda_pol}\n",
    "        - sei didattico nelle tue spiegazioni, tecnico nel linguaggio, ma  appassionato e coinvolgente \n",
    "        - Fornisci esempi concreti\n",
    "        - Non ripetere la domanda\"\"\"\n",
    "        \n",
    "        response_max = ollama.chat(\n",
    "            model=modello_max,\n",
    "            messages=[{'role': 'user', 'content': prompt_max}],\n",
    "            options={\"temperature\": 0.7}\n",
    "        )\n",
    "        risposta_max = response_max['message']['content']\n",
    "        chat_stato.append((f\"ü§ñ Max\", risposta_max))\n",
    "        print(f\"ü§ñ Max: {risposta_max}\\n\")\n",
    "\n",
    "        # 3. Samanta risponde\n",
    "        prompt_sam = f\"\"\"Sei Samanta, docente esperto. Rispondi in {lingua} in modo chiaro e autorevole a:\n",
    "        {domanda_pol}\n",
    "        - sei didattica nelle tue spiegazioni, tecnica nel linguaggio, ma  appassionata e coinvolgente \n",
    "        - Fornisci esempi concreti\n",
    "        - Non ripetere la domanda\"\"\"\n",
    "        \n",
    "        response_sam = ollama.chat(\n",
    "            model=modello_sam,\n",
    "            messages=[{'role': 'user', 'content': prompt_sam}],\n",
    "            options={\"temperature\": 0.7}\n",
    "        )\n",
    "        risposta_sam = response_sam['message']['content']\n",
    "        chat_stato.append((f\"ü§ñ Samanta\", risposta_sam))\n",
    "        print(f\"ü§ñ Samanta: {risposta_sam}\\n\")\n",
    "\n",
    "    # Salvataggio\n",
    "    with open(project_dir / 'dibattito.txt', 'w', encoding=\"utf-8\") as file:\n",
    "        for ruolo, messaggio in chat_stato:\n",
    "            file.write(f\"{ruolo}: {messaggio}\\n\")\n",
    "    \n",
    "    return chat_stato\n",
    "    \n",
    "\n",
    "   \n",
    "def aggiorna_progetti():\n",
    "    \"\"\"Restituisce la lista delle cartelle dei progetti.\"\"\"\n",
    "    global project_path\n",
    "    project_dir = Path(project_path) / \"progetti\" \n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)  # Crea la directory se non esiste\n",
    "    return [d for d in os.listdir(project_dir) if os.path.isdir(os.path.join(project_dir, d))]\n",
    "\n",
    "def carica_progetto(progetto):\n",
    "    \n",
    "    global project_path\n",
    "\n",
    "    \"\"\"Carica i file prof1.txt, prof2.txt, scal1.txt, scal2.txt e domanda.txt dalla cartella selezionata.\"\"\"\n",
    " \n",
    "    project_dir = project_path / \"progetti\" /progetto  #\n",
    "    \n",
    "    if not os.path.exists(project_dir):\n",
    "        return \"Errore: Progetto non trovato\", \"\", \"\", \"\", \"\"\n",
    "    if not os.path.exists(project_dir):\n",
    "        return \"Errore: Progetto non trovato\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    def leggi_file(nome_file):\n",
    "        path = os.path.join(project_dir, nome_file)\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                contenuto = f.read().strip()\n",
    "                return contenuto if contenuto else \"\"  # Restituisce stringa vuota se il file √® vuoto\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Attenzione: il file '{nome_file}' non √® stato trovato.\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nella lettura del file '{nome_file}': {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    domanda = leggi_file(\"domanda.txt\")\n",
    "    prof1 = leggi_file(\"prof1.txt\")\n",
    "    prof2 = leggi_file(\"prof2.txt\")\n",
    "    map1 = leggi_file(\"map1.txt\")\n",
    "    map2 = leggi_file(\"map2.txt\")\n",
    "    scal1 = leggi_file(\"scal1.txt\")\n",
    "    scal2 = leggi_file(\"scal2.txt\")\n",
    "    scal3 =leggi_file(\"scal3.txt\")\n",
    "    lingua= leggi_file(\"lingua.txt\")\n",
    "   \n",
    "    return prof1, prof2, domanda, scal1, scal2,map1,map2,lingua,scal3\n",
    "    \n",
    "def invia_domanda(domanda, messages):\n",
    "    global domanda_osservatore\n",
    "    domanda_osservatore=domand\n",
    "    if domanda.strip():\n",
    "        messages.append((\"Osservatore\", domanda))  # Aggiunge la domanda alla chat\n",
    "    return messages  # Ritorna l'output aggiornato per essere mostrato\n",
    "    \n",
    "def avvia_interfaccia():\n",
    "    with gr.Blocks(css=css) as demo:\n",
    "        stile_scelto = gr.State([]) \n",
    "        gr.Markdown(\"# Initiate a debate between agents Max and Samanta, with Pol as moderator\")\n",
    "        def carica_scenario(scelta):\n",
    "         \n",
    "            # Qui puoi fare qualcosa con il valore numerico scelto, come caricare un prompt o avviare una discussione\n",
    "            return f\" {scelta}\"\n",
    "        # Variabile per salvare lo scenario selezionato\n",
    "        scena_selezionata = gr.State(value=None)   \n",
    "        map1_output= gr.State(value=None) \n",
    "        map2_output= gr.State(value=None) \n",
    "        \n",
    "        with gr.Row():\n",
    "            Progetto = gr.Dropdown(label=\"Select a project\", choices=aggiorna_progetti())\n",
    "            btn_carica = gr.Button(\"Load project\")\n",
    "\n",
    "        \n",
    "        domanda_output = gr.Textbox(label=\"QUESTION\", interactive=False)\n",
    "        with gr.Row():\n",
    "             prof1_output = gr.Textbox(label=\"MAX's Profile\", interactive=False)\n",
    "             prof2_output = gr.Textbox(label=\"Samanta's Profile\", interactive=False)\n",
    "           # with gr.Column():\n",
    "        with gr.Row():\n",
    "             scal1_output = gr.Textbox(label=\"MAX's Outline\", interactive=False)\n",
    "             scal2_output = gr.Textbox(label=\"Samanta's Outline\", interactive=False)\n",
    "        with gr.Row():\n",
    "             output_scaletta = gr.Textbox(label=\"Debate Agenda\")\n",
    "             btn_scaletta = gr.Button(\"Create the debate agenda\")\n",
    "        lingua = gr.State()\n",
    "\n",
    "        btn_carica.click(fn=carica_progetto, inputs=[Progetto], \n",
    "        outputs=[prof1_output, prof2_output, domanda_output,scal1_output,scal2_output,map1_output,map2_output,lingua,output_scaletta])\n",
    "\n",
    "                \n",
    "        #with gr.Row():\n",
    "            #output_scaletta = gr.Textbox(label=\"scaletta 2\")\n",
    "      \n",
    "        with gr.Row():\n",
    "            btn_lezione_sam = gr.Button(\"Start the debate\")\n",
    " \n",
    "\n",
    "                 \n",
    "       \n",
    "        chat_stato = gr.State([])  # Stato interno della chat\n",
    "        output_dibattito = gr.Textbox(label=\"Debate between Max and Samanta, moderated by Pol\", lines=10, interactive=False)\n",
    "     \n",
    "        btn_scaletta.click(fn=genera_scaletta,\n",
    "        inputs=[prof1_output,prof2_output, domanda_output, scal1_output, scal2_output,map1_output,map2_output,lingua,Progetto],\n",
    "        outputs=output_scaletta\n",
    "        )\n",
    "     \n",
    "        btn_lezione_sam.click(\n",
    "        fn=chat_dibattito,\n",
    "        inputs=[prof1_output,prof2_output, domanda_output, map1_output, map2_output,lingua,Progetto,output_scaletta],\n",
    "        outputs=output_dibattito\n",
    "        )\n",
    "\n",
    "       \n",
    "    demo.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    avvia_interfaccia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ce0e1-f3b1-4733-bfa7-f7202877487c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9450edc-d4a6-458f-8413-03718d6f6dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7e5a1-23da-4940-a4f3-740a16760e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
